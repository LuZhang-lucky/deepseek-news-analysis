{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f034105b-925d-451d-a941-2195980a6923",
   "metadata": {},
   "source": [
    "Now I will perform topic modeling by using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2688fcf-005f-4266-8aff-ee556f450dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/lulu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk  # Import the nltk module\n",
    "\n",
    "nltk.download('wordnet')  \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40327659-d9be-4ce4-8acb-32774f3c22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Step 1: Read the preprocessed text from a file\n",
    "# Assuming each line in the file is a separate document and words are separated by spaces\n",
    "file_path = \"./deepseek_output_lemmas.txt\"  \n",
    "\n",
    "# Read the file and create a list of tokenized documents (list of words)\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    text_list = [line.strip().split() for line in file.readlines()]  # Tokenize each line by space\n",
    "\n",
    "# Step 2: Create a Dictionary\n",
    "dictionary = corpora.Dictionary(text_list)\n",
    "\n",
    "# Step 3: Create a Bag-of-Words Corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_list]\n",
    "\n",
    "# Step 4: Train the LDA Model\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=20)\n",
    "\n",
    "# Step 5: Print Topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Optional: Evaluate coherence score (optional but recommended)\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=text_list, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(\"Coherence Score: \", coherence_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f38798-be89-42fe-bdc7-28d8e6ac7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add irrelevant words to stop_words\n",
    "additional_stopwords = {\"said\", \"u\", \"company\", \"deepseeks\", \"tech\",\"say\",\"china\",\"chinese\"}\n",
    "stop_words = set(stopwords.words('english')).union(additional_stopwords)\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation/numbers\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d804916-c465-4180-a5e2-0971c51104fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Create a dictionary and a corpus from the tokenized data\n",
    "dictionary = Dictionary(df['tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "# Print the dictionary and corpus to check if everything looks fine\n",
    "print(dictionary.token2id)  # Check word-to-ID mapping\n",
    "print(corpus[:2])  # Check corpus (list of word counts per document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d04145a-5f67-4ab8-ab8f-34f4ffd8c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=30)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=5)  # Print the top 5 words for each topic\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a632532-2dbe-4e8d-a3cc-a04dc7fb70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass', dictionary=dictionary)\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score (u_mass): {coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7dcd7-ba8b-450f-85ba-727cf67c79c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coherence score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=df['tokens'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_lda}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1429c4-2293-4800-a3aa-95f482c10825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to remove the word deepseek ,ai,china,since they are meaningless, what do you think? I acuually expect topic like \"tech\"\"politic\"\"economy\"words like \"deepseek,\" \"ai,\" and \"china\" may dominate the topics and make them overly specific to certain aspects of your dataset, especially if they don't align with the broader themes you're aiming for, like tech, politics, and economy.\n",
    "#If you're looking to obtain more general topics like tech, politics, and economy, removing these highly frequent and specific words can definitely help the model focus on more meaningful, diverse, and interpretable topics.\n",
    "#Modify the Preprocessing to Exclude Specific Words:\n",
    "#To ensure words like \"deepseek,\" \"ai,\" and \"china\" are excluded from the topics, you can add them to a custom stopwords list and remove them during the preprocessing stage.\n",
    "custom_stopwords = stop_words.union({\"deepseek\", \"ai\", \"china\", \"chinese\", \"openai\"})\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation/numbers\n",
    "    tokens = [word for word in tokens if word not in custom_stopwords]  # Remove custom stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830bd50-5962-48b0-9b04-050f39be53fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c1764-897d-4fb2-add2-52c7e246ceb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (numpy_env)",
   "language": "python",
   "name": "numpy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
