{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4080b21f-ef34-41b3-ba70-c7e8178428d1",
   "metadata": {},
   "source": [
    "I want to output words frequency.I already pro-processed texts and use directly the lemmad text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16e6c9d4-c2d1-4383-9b29-0a32a1a5b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e436dcb7-c71a-407d-9232-b8e9b032f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "uk\n",
      "able\n",
      "resist\n",
      "china\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the relative file path\n",
    "file_path = Path(\"./deepseek_output_lemmas.txt\")\n",
    "\n",
    "# Open the file using Path\n",
    "with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Print data type and first 20 characters\n",
    "print(type(text))  # Check data type\n",
    "print(text[:20])  # Print first 20 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14c2aa0-a452-4628-bff8-d1df1a3d38e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['uk', 'able', 'resist', 'china', 'tech', 'dominance', 'china', 'success', 'technology', 'come', 'thin', 'air', 'give', 'unlikely', 'origin', 'deepseek', 'deep', 'shock', 'obscure', 'hangzhou']\n"
     ]
    }
   ],
   "source": [
    "text = text.split() #convert string to list,It splits the string into a list of words, using whitespace\n",
    "print(type(text))  # Check data type\n",
    "print(text[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e729aa6b-4280-47a1-876f-c216f9337198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ai</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>company</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>model</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>say</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>china</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>chinese</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tech</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>chip</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>like</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>technology</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>new</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>build</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>openai</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>world</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>app</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>year</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>government</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>question</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cost</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  Frequency\n",
       "14     deepseek        606\n",
       "41           ai        405\n",
       "114     company        305\n",
       "243       model        305\n",
       "213         say        301\n",
       "3         china        287\n",
       "52      chinese        262\n",
       "4          tech        219\n",
       "288        chip        150\n",
       "55         like        146\n",
       "7    technology        144\n",
       "78          new        127\n",
       "310       build        122\n",
       "655      openai        109\n",
       "61        world         99\n",
       "602         app         99\n",
       "137        year         99\n",
       "210  government         94\n",
       "70     question         94\n",
       "26         cost         87"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count word frequencies\n",
    "word_freq = Counter(text)\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "df = pd.DataFrame(word_freq.items(), columns=['Word', 'Frequency'])\n",
    "df = df.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Display top 20 words\n",
    "df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0767a-90ac-4991-94f1-985408d67062",
   "metadata": {},
   "source": [
    "What can we do with the words frequency?In my practical process, I check words frequecy on voyant tool and antconc, and when I check it in voyant tool, I see words with green and red color, later I figruer out the color means sentiment,green means positive and red means negative,This inspired me to further research: what positive words and negative words the media use for discribing \"deepseek\"? and the paper\"history text\"also inspired me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c22cf43e-28af-49a8-ac32-c08794096aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /usr/local/anaconda3/envs/numpy_env/lib/python3.9/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/anaconda3/envs/numpy_env/lib/python3.9/site-packages (from openpyxl) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/anaconda3/envs/numpy_env/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23686d97-5744-4214-9d71-3e84068d60b3",
   "metadata": {},
   "source": [
    "I want to export the word frequency table to an Excel file using Pandas so I can manually edit it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2ef7973-54db-4840-86af-1a406dc6b94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file 'word_frequencies.xlsx' has been saved!\n"
     ]
    }
   ],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(word_freq.items(), columns=['Word', 'Frequency'])\n",
    "df = df.sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Export to Excel\n",
    "df.to_excel(\"word_frequencies.xlsx\", index=False, engine='openpyxl')\n",
    "\n",
    "print(\"Excel file 'word_frequencies.xlsx' has been saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36dca69-0872-49ae-b0e0-869284e1b243",
   "metadata": {},
   "source": [
    "get sentiment score for each word, so we can view the words with sentiment and frequency the same time.we apply sentiment analysis to each word,polarity score ranges from -1 to 1, and remove newtral words(score=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baa4985b-9fae-420d-896d-c03bce752659",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The `text` argument passed to `__init__(text)` must be a string, not <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Apply sentiment analysis to each word\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment Score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWord\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_sentiment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Remove neutral words (score = 0)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment Score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/numpy_env/lib/python3.9/site-packages/pandas/core/series.py:4917\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/numpy_env/lib/python3.9/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/numpy_env/lib/python3.9/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/numpy_env/lib/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/numpy_env/lib/python3.9/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m, in \u001b[0;36mget_sentiment\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sentiment\u001b[39m(word):\n\u001b[0;32m----> 9\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msentiment\u001b[38;5;241m.\u001b[39mpolarity  \u001b[38;5;66;03m# Polarity score ranges from -1 to 1\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/numpy_env/lib/python3.9/site-packages/textblob/blob.py:371\u001b[0m, in \u001b[0;36mBaseBlob.__init__\u001b[0;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    361\u001b[0m     text,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m     clean_html\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    369\u001b[0m ):\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, basestring):\n\u001b[0;32m--> 371\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    372\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `text` argument passed to `__init__(text)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    373\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    374\u001b[0m         )\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_html:\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    377\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_html has been deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    378\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo remove HTML markup, use BeautifulSoup\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    379\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_text() function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    380\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: The `text` argument passed to `__init__(text)` must be a string, not <class 'float'>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load word frequency table from Excel\n",
    "df = pd.read_excel(\"word_frequencies.xlsx\")\n",
    "\n",
    "# Define a function to get sentiment scores using TextBlob\n",
    "def get_sentiment(word):\n",
    "    score = TextBlob(word).sentiment.polarity  # Polarity score ranges from -1 to 1\n",
    "    return score\n",
    "\n",
    "# Apply sentiment analysis to each word\n",
    "df[\"Sentiment Score\"] = df[\"Word\"].apply(get_sentiment)\n",
    "\n",
    "# Remove neutral words (score = 0)\n",
    "df = df[df[\"Sentiment Score\"] != 0]\n",
    "\n",
    "# Sort by sentiment score\n",
    "df = df.sort_values(by=\"Sentiment Score\", ascending=False)\n",
    "\n",
    "# Export the filtered table to a new Excel file\n",
    "df.to_excel(\"sentiment_word_frequencies_textblob.xlsx\", index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(\"Filtered sentiment table saved as 'sentiment_word_frequencies_textblob.xlsx'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1436f8b-b799-42a8-be98-3caef04a117b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's visualize the result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select top 10 positive and negative words\n",
    "top_pos = df.nlargest(10, \"Sentiment Score\")\n",
    "top_neg = df.nsmallest(10, \"Sentiment Score\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.barh(top_pos[\"Word\"], top_pos[\"Sentiment Score\"], color=\"green\", label=\"Positive\")\n",
    "plt.barh(top_neg[\"Word\"], top_neg[\"Sentiment Score\"], color=\"red\", label=\"Negative\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.title(\"Top Positive and Negative Words\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4b7c5-c9e7-4dcc-93ec-332b60abc833",
   "metadata": {},
   "source": [
    "Here, we have to evaluate the result. Since machine aotomatly filtered a set of words, so when we look back to the words frequency table, some wimportant words are missing.like \"innovation\"\"create\"\"ban\".\n",
    "as we can see clearly,\"shocking\" and \"shockingly\", the machine give them -1.0, means extremely negative, but in the context, we are not sure it's a negative word, for deepseek is defnertly a extremely possitive word with score +1.0, so this demonstrante that we can not rely on the tool analysis completely.And as I interprete, I consider the phnomonon is a breakthrough so I assign manually the score +1.0 to \"shocking\", and I combine the frequency of two words \"shockingly\"\"shocking\"'. and words frequency also matters, so innovation(freqency47)is also added  to the sentiment analysis table,and I assign the score 1.0 to the word \"innovation\".added \"ban\" (frequency 40) with score -0.8.starup(47)with score0.3.lead(77) with score 0.2,create(42)score0.7, there are conducted manually.the modified table was restored with the name\"sentiment_word_frequencies_textblob_man.xlsx\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda54fdf-15b8-4449-bcf5-fa645f9deca2",
   "metadata": {},
   "source": [
    "I am thinking about delete some words which with low frequency and low scores, since too tired doing it manually. why frequecy below 20?I visualize the whole data first, and notice that the scale of words with frequecy above 20 is large enough.so I set 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e8ed14-5475-4add-ba5f-115e94fa348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"./sentiment_word_frequencies_textblob_man.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Ensure numeric values for filtering\n",
    "df[\"Frequency\"] = pd.to_numeric(df[\"Frequency\"], errors=\"coerce\")\n",
    "df[\"Sentiment Score\"] = pd.to_numeric(df[\"Sentiment Score\"], errors=\"coerce\")\n",
    "\n",
    "# Remove words with frequency < 20 and sentiment score < 0.5\n",
    "df_filtered = df[(df[\"Frequency\"] >20) | (df[\"Sentiment Score\"] >= 0.5)]\n",
    "\n",
    "# Save the cleaned data to a new file\n",
    "df_filtered.to_excel(\"./positive_sentiment_words.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ Done! The filtered words are saved in 'positive_sentiment_words.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b3d184-ac63-4713-9ad7-f54e60e167a1",
   "metadata": {},
   "source": [
    "after filtering It's still has 81 words left,it's still too much for viralization.I change some score manually, the aim is to show them , I think these adjectiv words is valuable, so I change the score from0.5 to 0.6, so that I can adjust parament to 0.6(higher). I also combine some words which have the same root:\"innovative\"\"innovation\",\"develop\"and\"development\",\"success\"\"successful\". Adjust parament is a hard work, and I spen a lot of time on this step, the aim is to filter the most valuble data, but what is valuable, apparently the precess mixed with my subjectivty.I also add words like\"breakthrough\"(frequency 30),power(79),raise(27),cheap(28)\n",
    "simutaniously, I delete some words which has high score but meaningless in this contex, like\"honest\",frequency just once ,and score 0.6, I don't want show it so delete it.and score of\"welcome\" is 0.8, when it's done, we have to restore the result as\"positive_sentiment_words_man\", otherwise when we run the code again, the excle file will rewrite.\n",
    "Firstly, we consider frequency and sentiment score these two weight, so we want to show the words in two dimentional space. And secondly we will show the words with high sentiment score.So in this first table, we just consider words with frequency above 20 and sentiment score above0.6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b067673-992b-4392-83db-5bb9e93ccd62",
   "metadata": {},
   "source": [
    "We need check the result and delete some meaningless words manally,now the results reduced to 28 words, and we can visualize the result.when we check the table, there are many words have the same score, it leads to the problem that these points will be concentrated together and difficult to distinguish.so we manually chage the score slittly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d32a2-6dee-426f-acbf-818b22ebe0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read data from Excel file\n",
    "file_path = './positive_sentiment_words_man.xlsx'  \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(df['Frequency'], df['Sentiment Score'])\n",
    "\n",
    "# Add labels to each point with offset\n",
    "for i, row in df.iterrows():\n",
    "    offset_x = np.random.uniform(-1, 1)\n",
    "    offset_y = np.random.uniform(-0.02, 0.02)\n",
    "    plt.text(row['Frequency'] + offset_x, row['Sentiment Score'] + offset_y, row['Word'], fontsize=6, ha='center', va='bottom')\n",
    "\n",
    "# Set plot title and axis labels\n",
    "plt.title('Word Frequency vs Sentiment Score')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Sentiment Score')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce55301-8e53-4d1b-ac61-806a34105e78",
   "metadata": {},
   "source": [
    "We try different visualization,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2a6f5-b616-4359-8477-d0b5a4820d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read data from Excel file\n",
    "file_path = './positive_sentiment_words_man.xlsx' \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Sort by Sentiment Score (descending) and take top 20 words\n",
    "df_top = df.sort_values(by=\"Sentiment Score\", ascending=False).head(20)\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(df_top[\"Word\"], df_top[\"Sentiment Score\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.title(\"Top 20 Positive Words with Highest Sentiment Scores\")\n",
    "plt.gca().invert_yaxis()  # Invert Y-axis for better readability\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc57249-cb3f-4716-81f8-1d6c655f27bd",
   "metadata": {},
   "source": [
    "Now let's do the negative words part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f44ad-5344-4848-b240-549f46b16130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(\"./sentiment_word_frequencies_textblob_man.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Ensure numeric values for filtering\n",
    "df[\"Frequency\"] = pd.to_numeric(df[\"Frequency\"], errors=\"coerce\")\n",
    "df[\"Sentiment Score\"] = pd.to_numeric(df[\"Sentiment Score\"], errors=\"coerce\")\n",
    "\n",
    "# Filter the data\n",
    "df_filtered = df[((df[\"Frequency\"] > 20) | (df[\"Sentiment Score\"] < - 0.5)) & (df[\"Sentiment Score\"] <= 0)]\n",
    "\n",
    "# Save the cleaned data to a new file\n",
    "df_filtered.to_excel(\"./negative_sentiment_words.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ Done! The filtered words are saved in 'negative_sentiment_words.xlsx'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6b7f23-5a71-4646-b953-a1bcd6a06c26",
   "metadata": {},
   "source": [
    "the result has 20 words, so it's possiable for virualization.I also add some words which I considered are meaningful.like\"risk\"(frequency22),\"security\"(frequency31),rival(34),and we saved it as \"2negative_sentiment_words.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cb1b46-ed9d-455e-b18b-0e51ca254699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Read data from Excel file\n",
    "file_path = './2negative_sentiment_words.xlsx'  \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(df['Frequency'], df['Sentiment Score'])\n",
    "\n",
    "# Add labels to each point with offset\n",
    "for i, row in df.iterrows():\n",
    "    offset_x = np.random.uniform(-1, 1)\n",
    "    offset_y = np.random.uniform(-0.02, 0.02)\n",
    "    plt.text(row['Frequency'] + offset_x, row['Sentiment Score'] + offset_y, row['Word'], fontsize=6, ha='center', va='bottom')\n",
    "\n",
    "# Set plot title and axis labels\n",
    "plt.title('Word Frequency vs Sentiment Score')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Sentiment Score')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773fb602-8f0b-4c96-87a2-1cc4d24cd33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read data from Excel file\n",
    "file_path = './2negative_sentiment_words.xlsx' \n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Sort by Sentiment Score (ascending) and take top 20 words\n",
    "df_top = df.sort_values(by=\"Sentiment Score\", ascending=True).head(20)\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(df_top[\"Word\"], df_top[\"Sentiment Score\"], color=\"red\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.title(\"Top 20 negative Words with lowest Sentiment Scores\")\n",
    "plt.gca().invert_yaxis()  # Invert Y-axis for better readability\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5413f5d-6aaa-4faf-bb5a-155ddc5254aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the positive words Excel file\n",
    "positive_file_path = './2positive_sentiment_words.xlsx'\n",
    "positive_df = pd.read_excel(positive_file_path)\n",
    "\n",
    "# Read the negative words Excel file\n",
    "negative_file_path = './2negative_sentiment_words.xlsx'  \n",
    "negative_df = pd.read_excel(negative_file_path)\n",
    "\n",
    "# Convert 'Sentiment Score' to numeric (force errors to NaN) and drop NaN values for positive words\n",
    "positive_df[\"Sentiment Score\"] = pd.to_numeric(positive_df[\"Sentiment Score\"], errors='coerce')\n",
    "positive_df = positive_df.dropna(subset=[\"Sentiment Score\"])\n",
    "\n",
    "# Convert 'Sentiment Score' to numeric (force errors to NaN) and drop NaN values for negative words\n",
    "negative_df[\"Sentiment Score\"] = pd.to_numeric(negative_df[\"Sentiment Score\"], errors='coerce')\n",
    "negative_df = negative_df.dropna(subset=[\"Sentiment Score\"])\n",
    "\n",
    "# Plot the combined chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot positive sentiment words\n",
    "plt.barh(positive_df[\"Word\"][-10:], positive_df[\"Sentiment Score\"][-10:], color=\"green\", label=\"Positive\")\n",
    "\n",
    "# Plot negative sentiment words\n",
    "plt.barh(negative_df[\"Word\"][:10], negative_df[\"Sentiment Score\"][:10], color=\"red\", label=\"Negative\")\n",
    "\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.title(\"Top Positive and Negative Words with Sentiment Scores\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a27a51-c387-4968-b56f-4f8664be586e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (numpy_env)",
   "language": "python",
   "name": "numpy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
