{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f034105b-925d-451d-a941-2195980a6923",
   "metadata": {},
   "source": [
    "Now I will perform topic modeling by using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2688fcf-005f-4266-8aff-ee556f450dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/lulu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk  # Import the nltk module\n",
    "\n",
    "nltk.download('wordnet')  \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40327659-d9be-4ce4-8acb-32774f3c22a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.028*\"technology\" + 0.019*\"cost\" + 0.015*\"datum\" + 0.011*\"fall\" + 0.011*\"chatbot\" + 0.011*\"system\" + 0.011*\"giant\" + 0.010*\"way\" + 0.010*\"export\" + 0.010*\"Liang\"')\n",
      "(1, '0.023*\"lead\" + 0.021*\"build\" + 0.020*\"DeepSeeks\" + 0.017*\"week\" + 0.017*\"firm\" + 0.016*\"question\" + 0.016*\"Nvidia\" + 0.015*\"need\" + 0.014*\"Monday\" + 0.013*\"work\"')\n",
      "(2, '0.030*\"market\" + 0.017*\"new\" + 0.016*\"investor\" + 0.015*\"world\" + 0.015*\"app\" + 0.014*\"spend\" + 0.013*\"startup\" + 0.013*\"day\" + 0.012*\"investment\" + 0.010*\"powerful\"')\n",
      "(3, '0.052*\"tech\" + 0.036*\"chip\" + 0.027*\"like\" + 0.021*\"government\" + 0.018*\"industry\" + 0.015*\"call\" + 0.013*\"concern\" + 0.011*\"think\" + 0.011*\"include\" + 0.011*\"people\"')\n",
      "(4, '0.029*\"year\" + 0.023*\"power\" + 0.022*\"stock\" + 0.018*\"OpenAI\" + 0.018*\"billion\" + 0.014*\"american\" + 0.014*\"Meta\" + 0.013*\"open\" + 0.012*\"intelligence\" + 0.012*\"United\"')\n",
      "Coherence Score:  0.8193175667658913\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Step 1: Read the preprocessed text from a file\n",
    "# Assuming each line in the file is a separate document and words are separated by spaces\n",
    "file_path = \"../data/deepseek_output_lemmas.txt\"  \n",
    "\n",
    "# Read the file and create a list of tokenized documents (list of words)\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    text_list = [line.strip().split() for line in file.readlines()]  # Tokenize each line by space\n",
    "\n",
    "# Step 2: Create a Dictionary\n",
    "dictionary = corpora.Dictionary(text_list)\n",
    "\n",
    "# Step 3: Create a Bag-of-Words Corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_list]\n",
    "\n",
    "# Step 4: Train the LDA Model\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=20)\n",
    "\n",
    "# Step 5: Print Topics\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "# Optional: Evaluate coherence score (optional but recommended)\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=text_list, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model_lda.get_coherence()\n",
    "print(\"Coherence Score: \", coherence_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "172b40a3-f2b3-4162-90fd-37dc11c7b276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20577 documents\n",
      "Sample document tokens: ['UK']...\n",
      "\n",
      "Dictionary size: 4118 unique tokens\n",
      "Corpus size: 20577 documents\n",
      "First document in corpus: [(0, 1)]...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Load the lemmatized data from the text file\n",
    "file_path = \"../data/deepseek_output_lemmas.txt\"\n",
    "\n",
    "# Read the file - assuming each line contains tokens for one document\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    # Read all lines and strip whitespace\n",
    "    lines = [line.strip() for line in file.readlines()]\n",
    "    \n",
    "    # Convert each line to a list of tokens (assuming tokens are space-separated)\n",
    "    tokens_list = [line.split() for line in lines if line]  # Skip empty lines\n",
    "\n",
    "# Create a DataFrame with the tokens\n",
    "df = pd.DataFrame({'tokens': tokens_list})\n",
    "\n",
    "# Print basic info about the loaded data\n",
    "print(f\"Loaded {len(df)} documents\")\n",
    "print(f\"Sample document tokens: {df['tokens'][0][:10]}...\")  # Show first 10 tokens of first doc\n",
    "\n",
    "# Create a dictionary and a corpus from the tokenized data\n",
    "dictionary = Dictionary(df['tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "# Print dictionary and corpus info to check if everything looks fine\n",
    "print(f\"\\nDictionary size: {len(dictionary)} unique tokens\")\n",
    "print(f\"Corpus size: {len(corpus)} documents\")\n",
    "print(f\"First document in corpus: {corpus[0][:5]}...\")  # Show first 5 token-id pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f38798-be89-42fe-bdc7-28d8e6ac7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add irrelevant words to stop_words\n",
    "additional_stopwords = {\"said\", \"u\", \"company\", \"deepseeks\", \"tech\",\"say\",\"china\",\"chinese\"}\n",
    "stop_words = set(stopwords.words('english')).union(additional_stopwords)\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation/numbers\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d804916-c465-4180-a5e2-0971c51104fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoherencemodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoherenceModel\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create a dictionary and a corpus from the tokenized data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m dictionary \u001b[38;5;241m=\u001b[39m Dictionary(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m corpus \u001b[38;5;241m=\u001b[39m [dictionary\u001b[38;5;241m.\u001b[39mdoc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Print the dictionary and corpus to check if everything looks fine\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Create a dictionary and a corpus from the tokenized data\n",
    "dictionary = Dictionary(df['tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
    "\n",
    "# Print the dictionary and corpus to check if everything looks fine\n",
    "print(dictionary.token2id)  # Check word-to-ID mapping\n",
    "print(corpus[:2])  # Check corpus (list of word counts per document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d04145a-5f67-4ab8-ab8f-34f4ffd8c0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.028*\"year\" + 0.019*\"DeepSeeks\" + 0.018*\"industry\" + 0.017*\"new\" + 0.016*\"world\"')\n",
      "(1, '0.028*\"technology\" + 0.015*\"week\" + 0.011*\"concern\" + 0.011*\"system\" + 0.010*\"fall\"')\n",
      "(2, '0.034*\"chip\" + 0.021*\"lead\" + 0.021*\"build\" + 0.021*\"government\" + 0.017*\"billion\"')\n",
      "(3, '0.030*\"market\" + 0.022*\"power\" + 0.016*\"question\" + 0.016*\"app\" + 0.014*\"american\"')\n",
      "(4, '0.051*\"tech\" + 0.027*\"like\" + 0.021*\"stock\" + 0.020*\"cost\" + 0.019*\"OpenAI\"')\n"
     ]
    }
   ],
   "source": [
    "# Train the LDA model\n",
    "lda_model = LdaModel(corpus, num_topics=5, id2word=dictionary, passes=30)\n",
    "\n",
    "# Print the topics\n",
    "topics = lda_model.print_topics(num_words=5)  # Print the top 5 words for each topic\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a632532-2dbe-4e8d-a3cc-a04dc7fb70c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score (u_mass): -21.825769216702447\n"
     ]
    }
   ],
   "source": [
    "coherence_model = CoherenceModel(model=lda_model, corpus=corpus, coherence='u_mass', dictionary=dictionary)\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Coherence Score (u_mass): {coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7f7dcd7-ba8b-450f-85ba-727cf67c79c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.8191685080609645\n"
     ]
    }
   ],
   "source": [
    "# Calculate coherence score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=df['tokens'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_lda}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1429c4-2293-4800-a3aa-95f482c10825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I want to remove the word deepseek ,ai,china,since they are meaningless, what do you think? I acuually expect topic like \"tech\"\"politic\"\"economy\"words like \"deepseek,\" \"ai,\" and \"china\" may dominate the topics and make them overly specific to certain aspects of your dataset, especially if they don't align with the broader themes you're aiming for, like tech, politics, and economy.\n",
    "#If you're looking to obtain more general topics like tech, politics, and economy, removing these highly frequent and specific words can definitely help the model focus on more meaningful, diverse, and interpretable topics.\n",
    "#Modify the Preprocessing to Exclude Specific Words:\n",
    "#To ensure words like \"deepseek,\" \"ai,\" and \"china\" are excluded from the topics, you can add them to a custom stopwords list and remove them during the preprocessing stage.\n",
    "custom_stopwords = stop_words.union({\"deepseek\", \"ai\", \"china\", \"chinese\", \"openai\"})\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation/numbers\n",
    "    tokens = [word for word in tokens if word not in custom_stopwords]  # Remove custom stopwords\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830bd50-5962-48b0-9b04-050f39be53fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c1764-897d-4fb2-add2-52c7e246ceb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (numpy_env)",
   "language": "python",
   "name": "numpy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
